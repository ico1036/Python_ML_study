{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# For drawing sigmoid function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Binaray classification\n",
    "__Binary__ or __binomial classification__ is the task of classifying the elements of a given set into two groups (predicting which group each one belongs to) \n",
    "  on the basis of a classification rule.  \n",
    "The __Logistic Classifiaction__ is the go-to method for binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample data\n",
    "x_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]]\n",
    "y_data = [[0],[0],[0],[1],[1],[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]),name='biaas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hypothesis: Sigmoid function(Logistic function)\n",
    " - #### We need to change the old hypothesis function: $$Z = XW+b$$  \n",
    " X: training input data, W: weight, b:  bias  \n",
    " This function can generate output values of extremely higher than 1 or lower than 0, so this fuction is not fit with the binary classification \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - #### New Hypothesis function: $$H(Z) ={1\\over(1+e^{-z})}$$\n",
    " Z means the old hypothesis function. This shape is decribedd below\n",
    " This function allow all values transform into [-1,1]\n",
    " This new hypothesis function is called \"sigmoid\" or \"Logistic\"\n",
    " - #### True if the hypothesis > 0.5 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## -- Sample code for sigmoid function\n",
    "def sigmoid(x):\n",
    "    a=[]\n",
    "    for item in x:\n",
    "        a.append(1/(1+math.exp(-item)))\n",
    "    return a\n",
    "\n",
    "x = np.arange(-10., 10., 0.2)\n",
    "sig = sigmoid(x)\n",
    "plt.plot(x,sig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypothesis = tf.sigmoid(tf.matmul(X,W)+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Loss function\n",
    " - #### We also need to change the old loss function:$$ loss(w) = \\frac{1}{m} \\sum_{i=1}^m (H(x^i)-y^i)^2$$\n",
    " When you use the Gradient Descent method, this function makes irregular shape( It contains many local holes ) because the new hypothesis function is non-linear. Threrfore you may be traped  in those holes and output wrong training results\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - New loss fuction: $$loss(w) = \\frac{1}{m} \\sum_{i=1}^m C(H(x),y)$$ \n",
    "\n",
    "$$C(H(x),y)=\\left\\{\\begin{matrix} -log(H(x)), & \\mbox{if }y\\mbox{ =1} \\\\ -log(1-H(x)), & \\mbox{if }y\\mbox{ =0} \\end{matrix}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - This function is simply integrated as one formula:   \n",
    "   \n",
    "   $$C(H(x),y) = -ylog(H(x))-(1-y)log(1-H(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "# Accuracy and Prediction(T or F)\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        loss_val, _ = sess.run([loss,train],feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 1000 ==0:\n",
    "            print(step, loss_val)\n",
    "    print(\"#######Training end########\")\n",
    "    \n",
    "    #Accuracy report\n",
    "    h,c,a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow_",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
